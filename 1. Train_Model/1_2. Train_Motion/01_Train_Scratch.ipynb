{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Model training pipeline for motion HAR (IMU)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, BatchNormalization, MaxPooling1D, Dropout, Flatten, Dense, Activation, concatenate)\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import optimizers, backend as K\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT / \"..\" / \"..\" / \"..\" / \"HCAR\"))\n",
    "\n",
    "# GPU configuration\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Seeds for reproducibility\n",
    "SEED = 20\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Training settings\n",
    "MODEL_VERSION = 36\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "HAND = 'Right'\n",
    "SUB_SR = 16000\n",
    "IMU_SR = 50\n",
    "WINDOW_LEN_IMU = 2 * IMU_SR      # 2 seconds\n",
    "HOP_LEN_IMU    = int(0.2 * IMU_SR)  # 0.2s stride\n",
    "\n",
    "# Model input shape\n",
    "IMU_INPUT_SHAPE = (100, 9)\n",
    "\n",
    "# Participants\n",
    "TRAIN_PIDS = [10, 100, 101, 102, 103]\n",
    "VALID_PIDS = [104]\n",
    "TEST_PIDS = [3, 4]\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = Path(\"../../\")\n",
    "DATA_PATH = BASE_PATH / \"Data/Train_Data/3_MMExamples\"\n",
    "MODEL_SAVE_PATH = BASE_PATH / f\"Models/tensorflow_model/Motion/Motion_ver{MODEL_VERSION}/{HAND}\"\n",
    "ACC_SAVE_PATH = BASE_PATH / f\"../../Result/Model_Accuracy/Motion/Motion_ver{MODEL_VERSION}/{HAND}\"\n",
    "PRED_SAVE_PATH = BASE_PATH / f\"../../Result/Model_Preds/Motion/Motion_ver{MODEL_VERSION}/{HAND}\"\n",
    "NORM_PATH = BASE_PATH / f\"Normalization_params/Normalization_params_pickle/normalization_params_motion_{HAND}_ver{MODEL_VERSION}.pkl\"\n",
    "\n",
    "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "ACC_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PRED_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Functions\n",
    "def frame(data: np.ndarray, window_length: int, hop_length: int) -> np.ndarray:\n",
    "    # Frame the 1D/2D array into overlapping windows.\n",
    "    if data.shape[0] < window_length:\n",
    "        pad_len = window_length - data.shape[0]\n",
    "        pad = np.zeros((pad_len, ) + data.shape[1:], dtype=data.dtype)\n",
    "        data = np.concatenate([data, pad], axis=0)\n",
    "    n_frames = 1 + (data.shape[0] - window_length) // hop_length\n",
    "    shape = (n_frames, window_length) + data.shape[1:]\n",
    "    strides = (data.strides[0] * hop_length,) + data.strides[1:]\n",
    "    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "def compute_norm_params(train_ids: list) -> dict:\n",
    "    all_segments = []\n",
    "    for pid in train_ids:\n",
    "        folder = DATA_PATH / str(pid) / HAND / str(SUB_SR)\n",
    "        for file in tqdm(folder.glob('*.pkl'), desc=f\"Norm params pid={pid}\"):\n",
    "            data = pickle.load(open(file, 'rb'))['IMU']\n",
    "            if data.size:\n",
    "                all_segments.append(data)\n",
    "    concat = np.concatenate(all_segments, axis=0)\n",
    "    flat = concat.reshape(-1, concat.shape[-1])\n",
    "    return {\n",
    "        'max': np.percentile(flat, 80, axis=0),\n",
    "        'min': np.percentile(flat, 20, axis=0),\n",
    "        'mean': flat.mean(axis=0),\n",
    "        'std': flat.std(axis=0)\n",
    "    }\n",
    "\n",
    "def normalize_imu(data: np.ndarray, params: dict) -> np.ndarray:\n",
    "    # Normalize IMU data to [-1,1] then standardize to zero mean and unit std.\n",
    "\n",
    "    pmax = params['max'].reshape(1,1,-1)\n",
    "    pmin = params['min'].reshape(1,1,-1)\n",
    "    mean = params['mean'].reshape(1,1,-1)\n",
    "    std  = params['std'].reshape(1,1,-1)\n",
    "    scaled = 1 + (data - pmax) * 2 / (pmax - pmin)\n",
    "    return (scaled - mean) / std\n",
    "\n",
    "def load_dataset(pids: list, params: dict) -> tuple:\n",
    "    # Load and normalize IMU & audio data, return arrays and labels.\n",
    "    X_imu, y = [], []\n",
    "    for pid in pids:\n",
    "        folder = DATA_PATH / str(pid) / HAND / str(SUB_SR)\n",
    "        for file in tqdm(folder.glob('*.pkl'), desc=f\"Load pid={pid}\"):\n",
    "            meta = file.stem.split('---')\n",
    "            _, activity, _ = meta\n",
    "            data = pickle.load(open(file,'rb'))\n",
    "            imu = data['IMU'].astype(np.float32)\n",
    "\n",
    "            if imu.size:\n",
    "                imu = normalize_imu(imu, params)\n",
    "                X_imu.append(np.concatenate(imu, axis=0))\n",
    "                y.extend([[pid, activity]] * imu.shape[0])\n",
    "    return (\n",
    "        np.concatenate(X_imu, axis=0),\n",
    "        np.array(y)\n",
    "    )\n",
    "\n",
    "def data_generator(X_imu, y, batch_size, shuffle=True):\n",
    "    num_samples = X_imu.shape[0]\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "        else:\n",
    "            indices = np.arange(num_samples)\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_ids = indices[start:end]\n",
    "            yield X_imu[batch_ids], y[batch_ids]\n",
    "\n",
    "# Create IMU model\n",
    "def create_motion_model():\n",
    "\n",
    "    inputs = Input(shape=IMU_INPUT_SHAPE, name=\"IMU_input\")\n",
    "    x = Conv1D(filters=128, kernel_size=10, strides=1, padding='valid', activation='relu', name='Conv_1')(inputs)\n",
    "    x = BatchNormalization(name='batch_normalization_1')(x)\n",
    "\n",
    "    x = Conv1D(filters=128, kernel_size=10, strides=1, activation='relu', name='Conv_2')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_2')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='Max_pool_1')(x)\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=1, activation='relu', name='Conv_3')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_3')(x)\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=1, activation='relu', name='Conv_4')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_4')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='Max_pool_2')(x)\n",
    "    x = Dropout(rate=0.5, name='Dropout_1')(x)\n",
    "\n",
    "    x = Flatten(name='flatten')(x)\n",
    "\n",
    "    x = Dense(8192, activation='relu', name='dense_1')(x)\n",
    "    x = Dense(1024, activation='relu', name='dense_2')(x)\n",
    "    x = Dense(256, activation='relu', name='dense_3')(x)\n",
    "\n",
    "    outputs = Dense(6, activation='softmax', name='final_output')(x)\n",
    "\n",
    "    motion_model = Model(inputs=inputs, outputs=outputs, name='IMU_model')\n",
    "    adamOpt = optimizers.Adam(learning_rate=0.001)\n",
    "    motion_model.compile(optimizer=adamOpt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return motion_model\n",
    "\n",
    "def evaluate_and_save(model: Model, X_imu, y_labels, lb: LabelBinarizer):\n",
    "    # Perform filewise prediction, compute metrics, save results.\n",
    "    preds = model.predict(X_imu, batch_size=BATCH_SIZE)\n",
    "    df = pd.DataFrame(preds, columns=lb.classes_)\n",
    "    df['file_name'] = y_labels[:,1]\n",
    "    df['y_pred'] = df.drop(columns=['file_name']).idxmax(axis=1)\n",
    "    df['y_true'] = df['file_name']\n",
    "\n",
    "    ba = balanced_accuracy_score(df['y_true'], df['y_pred'])\n",
    "    f1 = f1_score(df['y_true'], df['y_pred'], average='weighted')\n",
    "    return df, ba, f1\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    # Compute or load normalization params\n",
    "    if NORM_PATH.exists():\n",
    "        params = pickle.load(open(NORM_PATH,'rb'))\n",
    "    else:\n",
    "        params = compute_norm_params(TRAIN_PIDS)\n",
    "        pickle.dump(params, open(NORM_PATH,'wb'))\n",
    "\n",
    "    # Load datasets\n",
    "    X_imu_tr, y_tr = load_dataset(TRAIN_PIDS, params)\n",
    "    X_imu_va, y_va = load_dataset(VALID_PIDS, params)\n",
    "    X_imu_te, y_te = load_dataset(TEST_PIDS, params)\n",
    "\n",
    "    # Encode labels and compute class weights\n",
    "    lb = LabelBinarizer().fit(y_tr[:,1])\n",
    "    y_tr_lbl = lb.transform(y_tr[:,1])\n",
    "    y_va_lbl = lb.transform(y_va[:,1])\n",
    "    cw = class_weight.compute_class_weight('balanced', classes=lb.classes_, y=y_tr[:,1])\n",
    "    cw_dict = dict(enumerate(cw))\n",
    "\n",
    "    # Build model\n",
    "    motion_model = create_motion_model()\n",
    "\n",
    "    # Train\n",
    "    reduce_lr = ReduceLROnPlateau('val_loss',0.1,3,verbose=1,min_lr=1e-6)\n",
    "    early_stop = EarlyStopping('val_loss',5,verbose=1,restore_best_weights=True)\n",
    "    train_gen = data_generator(X_imu_tr, y_tr_lbl, BATCH_SIZE, shuffle=True)\n",
    "    val_gen = data_generator(X_imu_va, y_va_lbl, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    motion_model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=[reduce_lr, early_stop],\n",
    "        class_weight=cw_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save model and accuracy\n",
    "    motion_model.save(MODEL_SAVE_PATH / 'Motion_Scratch.h5')\n",
    "\n",
    "    # Evaluate\n",
    "    results_df, ba, f1 = evaluate_and_save(motion_model, X_imu_te, y_te, lb)\n",
    "    results_df.to_csv(PRED_SAVE_PATH / f'{HAND}.csv', index=False)\n",
    "\n",
    "    with open(ACC_SAVE_PATH / 'MM_Scratch.txt', 'w') as f:\n",
    "        f.write(f'Balanced Accuracy: {ba}\\nF1 Score: {f1}\\n')\n",
    "\n",
    "    # Cleanup\n",
    "    del motion_model, X_imu_tr, y_tr, X_imu_va, y_va, X_imu_te, y_te\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_ver3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
