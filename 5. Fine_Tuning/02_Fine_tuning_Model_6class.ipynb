{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, clone_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOT = Path(\"../Data/Experiment_Data/3_PreprocessDataset_Oversample\")\n",
    "BASE_MODEL_PATH = Path(\"../Models/tensorflow_model/MultiModal/MultiModal_ver1/Right/MM_Scratch.h5\")\n",
    "LABEL_BINARIZER = Path(\"../LabelBinarizer/Label_binarizer_6_classes.pkl\")\n",
    "NORM_PARAMS = Path(\"../Normalization_params/Normalization_params_pickle/normalization_params_Right_ver1.pkl\")\n",
    "SAVE_BASE = Path(\"../Models/Finetuned_Model/Finetune_model_6class_ver3\")\n",
    "\n",
    "STRATEGIES = [\"full\", \"some\", \"one\"]\n",
    "TRAIN_SEC_LIST = [10, 30] + list(range(60, 481, 60))\n",
    "NOISE_SEC, VAL_SEC = 3, 10\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "CLASSES = [\"Shower\",\"Tooth_brushing\",\"Washing_hands\",\"Vacuum_Cleaner\",\"Wiping\",\"Other\"]\n",
    "SEED = 4\n",
    "\n",
    "# Utility Functions\n",
    "\n",
    "def compute_num_frames(duration_sec: float, window_length: float = 2.0, hop_length: float = 0.2):\n",
    "    \"\"\"Compute number of overlapping frames for given duration.\"\"\"\n",
    "    if duration_sec < window_length:\n",
    "        return 0\n",
    "    return 1 + int(math.floor((duration_sec - window_length) / hop_length))\n",
    "\n",
    "def split_train_val_idxs(labels: np.ndarray,\n",
    "                         train_frames: int,\n",
    "                         val_frames: int,\n",
    "                         noise_frames: int):\n",
    "    \"\"\"\n",
    "    For each class, split indices into training and validation segments.\n",
    "    \"\"\"\n",
    "    train_idxs, val_idxs = [], []\n",
    "    for cls in CLASSES:\n",
    "        cls_positions = np.where(labels == cls)[0]\n",
    "        if cls_positions.size == 0:\n",
    "            continue\n",
    "        # find contiguous segments\n",
    "        diffs = np.diff(cls_positions)\n",
    "        breaks = np.where(diffs != 1)[0]\n",
    "        seg_starts = np.concatenate(([0], breaks + 1))\n",
    "        seg_ends   = np.concatenate((breaks, [len(cls_positions) - 1]))\n",
    "\n",
    "        collected = []\n",
    "        for start, end in zip(seg_starts, seg_ends):\n",
    "            segment = cls_positions[start:end+1]\n",
    "            if segment.size > noise_frames:\n",
    "                collected.append(segment[noise_frames:])\n",
    "        if collected:\n",
    "            combined = np.concatenate(collected)\n",
    "        else:\n",
    "            combined = np.array([], dtype=int)\n",
    "\n",
    "        subset = combined[:train_frames + val_frames]\n",
    "        train_idxs.append(subset[:train_frames])\n",
    "        val_idxs.append(subset[train_frames:])\n",
    "\n",
    "    train_idxs = np.sort(np.concatenate(train_idxs)) if train_idxs else np.array([], dtype=int)\n",
    "    val_idxs   = np.sort(np.concatenate(val_idxs)) if val_idxs else np.array([], dtype=int)\n",
    "    return train_idxs, val_idxs\n",
    "\n",
    "def set_trainable_layers(model: tf.keras.Model, strategy: str):\n",
    "    \"\"\"Set layer.trainable based on fine-tuning strategy.\"\"\"\n",
    "    if strategy == 'full':\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "    elif strategy == 'some':\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = not any(key in layer.name for key in ['conv', 'batch_normalization', 'pool'])\n",
    "    elif strategy == 'one':\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        model.layers[-1].trainable = True\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "def normalize_imu_data(x: np.ndarray, norm: dict):\n",
    "    \"\"\"Apply min-max to [-1,1] then standardize IMU data.\"\"\"\n",
    "    pm, pn = norm['max'], norm['min']\n",
    "    mean, std = norm['mean'], norm['std']\n",
    "    scaled = 1 + (x - pm) * 2 / (pm - pn)\n",
    "    return ((scaled - mean) / std).astype('float32')\n",
    "\n",
    "# Main Fine-tuning Pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "    # Load base model, label binarizer, normalization params\n",
    "    base_model = load_model(BASE_MODEL_PATH)\n",
    "    with open(LABEL_BINARIZER, 'rb') as f:\n",
    "        lb = pkl.load(f)\n",
    "    with open(NORM_PARAMS, 'rb') as f:\n",
    "        norm = pkl.load(f)\n",
    "\n",
    "    # Precompute frames for noise and validation\n",
    "    noise_frames = compute_num_frames(NOISE_SEC)\n",
    "    val_frames = compute_num_frames(VAL_SEC)\n",
    "\n",
    "    for train_sec in TRAIN_SEC_LIST:\n",
    "        train_frames = compute_num_frames(train_sec)\n",
    "        for pid_dir in sorted(DATA_ROOT.iterdir()):\n",
    "            if not pid_dir.is_dir():\n",
    "                continue\n",
    "            pid = pid_dir.name\n",
    "            print(f\"\\n=== Fine-tuning {pid}, train_sec={train_sec} ===\")\n",
    "\n",
    "            # Load preprocessed data\n",
    "            with open(pid_dir / f\"{pid}_preprocessing.pkl\", 'rb') as f:\n",
    "                data = pkl.load(f)\n",
    "            imu_data = data['IMU']\n",
    "            audio_data = data['Audio']\n",
    "            labels = data['Activity']\n",
    "\n",
    "            # Split train/val indices\n",
    "            train_idxs, val_idxs = split_train_val_idxs(labels, train_frames, val_frames, noise_frames)\n",
    "\n",
    "            # Prepare datasets\n",
    "            X_imu_train = normalize_imu_data(imu_data[train_idxs], norm)\n",
    "            X_imu_val   = normalize_imu_data(imu_data[val_idxs], norm)\n",
    "            X_audio_train = audio_data[train_idxs].astype('float32')\n",
    "            X_audio_val   = audio_data[val_idxs].astype('float32')\n",
    "            y_train = lb.transform(labels[train_idxs])\n",
    "            y_val   = lb.transform(labels[val_idxs])\n",
    "\n",
    "            # Shuffle training data\n",
    "            perm = np.random.permutation(len(train_idxs))\n",
    "            X_imu_train = X_imu_train[perm]\n",
    "            X_audio_train = X_audio_train[perm]\n",
    "            y_train = y_train[perm]\n",
    "\n",
    "            # Iterate strategies\n",
    "            for strategy in STRATEGIES:\n",
    "                print(f\"-- Strategy: {strategy}\")\n",
    "                model = clone_model(base_model)\n",
    "                model.set_weights(base_model.get_weights())\n",
    "                set_trainable_layers(model, strategy)\n",
    "\n",
    "                # Dummy call for build\n",
    "                _ = model([tf.zeros((1,)+X_imu_train.shape[1:]), tf.zeros((1,)+X_audio_train.shape[1:])])\n",
    "                model.compile(optimizer=Adam(1e-4),\n",
    "                              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                # Callbacks\n",
    "                out_dir = SAVE_BASE / strategy / pid / f\"train{train_sec}\"\n",
    "                out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                checkpoint = ModelCheckpoint(str(out_dir / 'best.h5'), save_best_only=True, monitor='val_loss')\n",
    "                earlystop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "                # Fine-tune\n",
    "                model.fit(\n",
    "                    [X_imu_train, X_audio_train], y_train,\n",
    "                    validation_data=([X_imu_val, X_audio_val], y_val),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    callbacks=[checkpoint, earlystop], verbose=2\n",
    "                )\n",
    "\n",
    "                # Convert and save TFLite\n",
    "                converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "                converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "                tflite_model = converter.convert()\n",
    "                (out_dir / 'model.tflite').write_bytes(tflite_model)\n",
    "\n",
    "            print(f\"Completed fine-tuning for {pid} (train_sec={train_sec})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_ver3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
